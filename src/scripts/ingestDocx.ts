import fs from "fs";
import path from "path";
import dotenv from "dotenv";
import { OpenAIEmbeddings } from "@langchain/openai";
import { PineconeStore } from "@langchain/community/vectorstores/pinecone";
import { pineconeIndex } from "../services/pinecone";
import { chunkText } from "../utils/chunkText";
import { getTopicFromChunk } from "../utils/getTopic";
import mammoth from "mammoth";

dotenv.config();
// @ts-ignore
async function loadDocxText(filePath: string): Promise<string> {
  const result = await mammoth.extractRawText({ path: filePath });
  return result.value || "";
}

// üöÄ H√†m ch√≠nh ƒë·ªÉ chunk + embedding + ƒë·∫©y l√™n Pinecone
async function ingestDocx(filePath: string, namespace: string) {
  const rawText = await loadDocxText(filePath);
  console.log("üìÑ ƒê√£ ƒë·ªçc n·ªôi dung file DOCX. ƒêang chunk...");

  const chunks = chunkText(rawText, 1000);
  console.log(`üîπ Chunked th√†nh ${chunks.length} ƒëo·∫°n.`);

  const embeddings = new OpenAIEmbeddings();

  // T·∫°o metadata v·ªõi topic cho t·ª´ng chunk
  const metadatas = await Promise.all(
    chunks.map(async (chunk) => {
      const topic = await getTopicFromChunk(chunk);
      return { topic };
    })
  );

  await PineconeStore.fromTexts(chunks, metadatas, embeddings, {
    pineconeIndex,
    namespace,
  });

  console.log("‚úÖ ƒê√£ upload l√™n Pinecone th√†nh c√¥ng v·ªõi topic!");
}

// üìÇ ƒê∆∞·ªùng d·∫´n ƒë·∫øn file DOCX
const filePath = path.join(__dirname, "../../data/eco-residence.docx");
const namespace = "duan";

ingestDocx(filePath, namespace).catch((err) => {
  console.error("‚ùå L·ªói trong qu√° tr√¨nh ingest:", err);
});